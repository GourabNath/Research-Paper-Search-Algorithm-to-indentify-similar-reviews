{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Ronny/Documents/CAP/Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data(filename, review_column, rating_column):\n",
    "    \"\"\" \n",
    "        **Details of the <read_data> function**\n",
    "        The purpose of this function is to read a csv file into dataframe and perform stratified sampling of one column\n",
    "        based on another.\n",
    "        \n",
    "        Inputs : It takes 3 parameters.\n",
    "                    1. filename or path of the csv file\n",
    "                    2. column name containing the review text data\n",
    "                    3. column name containing the review rating data\n",
    "\n",
    "        Output : It will return two dataframes as output.\n",
    "                    1. sample dataframe\n",
    "                    2. test dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    review_df = pd.read_csv(filename)\n",
    "    \n",
    "    # Separating the Review text and the review rating columns\n",
    "    review_data = review_df[[review_column]]\n",
    "    rating_data = review_df[[rating_column]]\n",
    "    \n",
    "    # Stratified sampling for train and test data based on \"Review Rating\" (i.e. 1 to 5)\n",
    "    sample, test, _1, _2 = train_test_split( review_data, rating_data, test_size=0.33, random_state=42, stratify=rating_data)\n",
    "    \n",
    "    return sample, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample, test = read_data('C:/Users/Ronny/Documents/CAP/data/reviews.csv','Review Text', 'Review Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>I like so much .....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8803</th>\n",
       "      <td>It's very nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>Don't check the reviews just go for it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9959</th>\n",
       "      <td>Flagship Phone is here... Superb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7667</th>\n",
       "      <td>Excellent purchase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>Camera\\nSound\\nBattery\\nSpeed\\nAnd descent bod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>Best picture quality and sound quality with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>Hang too much.. Not worth expectation.. Could ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7518</th>\n",
       "      <td>It is a beast. And I got this at 27k + added b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Review Text\n",
       "8640                               I like so much .....\n",
       "8803                                     It's very nice\n",
       "8998             Don't check the reviews just go for it\n",
       "9959                Flagship Phone is here... Superb...\n",
       "7667                                Excellent purchase.\n",
       "4372  Camera\\nSound\\nBattery\\nSpeed\\nAnd descent bod...\n",
       "4291  Best picture quality and sound quality with th...\n",
       "2677  Hang too much.. Not worth expectation.. Could ...\n",
       "2297                                              Great\n",
       "7518  It is a beast. And I got this at 27k + added b..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning, Noun and Adjective extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en')\n",
    "import inflect\n",
    "inflect = inflect.engine()\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# Function to get the part of speech tagging for a word\n",
    "def get_pos(tok):\n",
    "    return tok.pos_ if ((str(tok) not in [\"rear\",\"front\",\"back\",\"sound\",\"backup\",\"mobile\"])) \\\n",
    "           else \"NOUN\" # <- Return whatever POS tag you'd like. \n",
    "\n",
    "# Function to extract singular nouns\n",
    "def singular(noun):\n",
    "    noun=str(noun)\n",
    "    if inflect.singular_noun(noun)==False:\n",
    "        return (noun)\n",
    "    else: return(inflect.singular_noun(noun))\n",
    "\n",
    "def lemmatize(word):\n",
    "    return lemmatizer(word, u'VERB')[0]\n",
    "\n",
    "def noun_lemma(review):\n",
    "    noun_sents=[]\n",
    "    parse=nlp(review.lower())\n",
    "    for i in parse.sents:\n",
    "        sentence=nlp(str(i))\n",
    "        noun_sents.append([j.lower_ for j in sentence if ((get_pos(j)==\"NOUN\") and (j.is_alpha) and (j.tag_!=\"WP\"))])\n",
    "    flatted_noun =list(set([y for x in noun_sents for y in x]))\n",
    "    lemma_noun=list(map(lemmatize,flatted_noun))\n",
    "    single_noun=list(map(singular,lemma_noun))\n",
    "    size_noun=list(set([x for x in single_noun if len(x)>2]))\n",
    "    return(','.join(size_noun))\n",
    "\n",
    "\n",
    "def create_transaction(review_text_data):\n",
    "    review=review_text_data\n",
    "    review_text=review.loc[:,\"Review Text\"]\n",
    "    transaction_doc=pd.DataFrame(columns=[\"Review_text\",\"Nouns\"])\n",
    "    transaction_doc[\"Review_text\"]=review_text\n",
    "    transaction_doc[\"Nouns\"]=transaction_doc[\"Review_text\"].apply(noun_lemma)\n",
    "    transaction_doc.to_csv(\"transaction_doc.csv\")\n",
    "    return(transaction_doc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction_data = create_transaction(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing python packages\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing mlxtend packages for sparse matrix & apriori\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "def apriori_func(noun_list,max_len):\n",
    "    \"\"\"\n",
    "\n",
    "    ** Details of the Apriori function **\n",
    "\n",
    "    The purpose of this function is to read the noun list and calculate the support for one and two nouns.\n",
    "\n",
    "    Inputs: It takes one input.\n",
    "            1. noun list dataframe\n",
    "\n",
    "    Outputs: It returns one output dataframe.\n",
    "            1. noun list with sorted support values.\n",
    "\n",
    "    \"\"\"\n",
    "    # Dropping the row which has no nouns and resetting the index\n",
    "    if 'Unnamed: 0' in noun_list.columns:\n",
    "        noun_list.drop(axis=1,columns='Unnamed: 0',inplace=True)\n",
    "    if noun_list.Nouns.isnull().sum()>0:\n",
    "        noun_list = noun_list.dropna()\n",
    "    elif (noun_list.Nouns == '').sum()>0:\n",
    "        noun_list = noun_list.loc[noun_list.Nouns != '',:]\n",
    "    else:\n",
    "        pass\n",
    "    noun_list.set_axis(range(noun_list.shape[0]),inplace=True)\n",
    "\n",
    "    # Preparing the dataframe as list of list for the sparse matrix\n",
    "    df_list = []\n",
    "    for i in range(noun_list.shape[0]):\n",
    "        df_list.append(str(noun_list.loc[i,'Nouns']).split(','))\n",
    "\n",
    "    #print(df_list[0:20])\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    te_ary1 = te.fit(df_list).transform(df_list)\n",
    "    df2 = pd.DataFrame(te_ary1, columns=te.columns_)\t\n",
    "\n",
    "    #print(df2.head(5))\n",
    "\n",
    "    apr = apriori(df2, min_support=0.01, use_colnames=True, max_len=max_len)\n",
    "    sorted_apr = apr.sort_values(['support'], ascending=False)\n",
    "\n",
    "    # print(sorted_apr)\n",
    "\n",
    "    temp=[]\n",
    "    sorted_apr['itemsets']=set(sorted_apr['itemsets'])\n",
    "    for i in sorted_apr['itemsets']:\n",
    "        temp.append(list(set(i)))\n",
    "    sorted_apr['itemsets']=temp\n",
    "    sorted_apr.to_csv(\"sorted_apriori.csv\")\n",
    "\n",
    "    return sorted_apr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app = apriori_func(transaction_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app1 = apriori_func(transaction_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(app1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
